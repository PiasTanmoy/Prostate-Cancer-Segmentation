{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-04T17:01:20.919588Z",
     "iopub.status.busy": "2025-07-04T17:01:20.919375Z",
     "iopub.status.idle": "2025-07-04T17:01:29.016085Z",
     "shell.execute_reply": "2025-07-04T17:01:29.015520Z",
     "shell.execute_reply.started": "2025-07-04T17:01:20.919568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import albumentations as A\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import SequentialLR, ConstantLR, CosineAnnealingWarmRestarts\n",
    "import random\n",
    "import albumentations as A\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import scipy.ndimage\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T18:51:55.369009Z",
     "iopub.status.busy": "2025-07-04T18:51:55.368329Z",
     "iopub.status.idle": "2025-07-04T19:06:27.966199Z",
     "shell.execute_reply": "2025-07-04T19:06:27.965187Z",
     "shell.execute_reply.started": "2025-07-04T18:51:55.368983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the code\n",
      "total number of parameters: 32550209\n",
      "total number of trainable parameters: 32550209\n",
      "Input shape : torch.Size([2, 3, 224, 224])\n",
      "Output shape : torch.Size([2, 6, 224, 224])\n",
      "Labeled Data Split: 176 training, 44 validation patients.\n",
      "\n",
      "--- STAGE 1: Starting Supervised Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/2449240925.py:820: FutureWarning:\n",
      "\n",
      "`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\n",
      "Finding training slices: 100%|██████████| 176/176 [00:01<00:00, 113.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2052 2D slices for this dataset partition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding validation slices: 100%|██████████| 44/44 [00:00<00:00, 458.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1056 2D slices for this dataset partition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding training slices: 100%|██████████| 44/44 [00:00<00:00, 109.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 508 2D slices for this dataset partition.\n",
      "\n",
      "--- Stage 1, Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 257/257 [01:02<00:00,  4.12it/s, loss=0.713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice (Balanced Set): 0.4663\n",
      "==> New best Stage 1 model saved with Lesion Dice: 0.4663\n",
      "\n",
      "--- Generating Pseudo-Labels for Stage 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pseudo-Masks: 100%|██████████| 205/205 [04:07<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: Starting Semi-Supervised Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding training slices: 100%|██████████| 205/205 [00:01<00:00, 139.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4458 2D slices for this dataset partition.\n",
      "Combining 2052 labeled slices with 4458 pseudo-labeled slices.\n",
      "\n",
      "--- Stage 2, Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 814/814 [03:16<00:00,  4.13it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice (Balanced Set): 0.4812\n",
      "==> New best Stage 2 model saved with Lesion Dice: 0.4812\n",
      "\n",
      "--- FINAL EVALUATION ---\n",
      "Loading best model from: /kaggle/working/semisupervised_output/best_semisupervised_model.pth\n",
      "\n",
      "Calculating Final 2D Metrics (on all validation slices)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2D Evaluation: 100%|██████████| 132/132 [00:20<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Final 3D Metrics (on validation volumes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3D Evaluation: 100%|██████████| 44/44 [02:42<00:00,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final 2D Slice-Level Metrics ---\n",
      "  Dice Score: 0.6403\n",
      "  AUROC:      0.9328\n",
      "  AP:         0.3407\n",
      "\n",
      "--- Final 3D Patient-Level Metrics ---\n",
      "  Dice Score: 0.2723\n",
      "  AUROC:      0.9069\n",
      "  AP:         0.3331\n",
      "\n",
      "Training finished. Best model at: /kaggle/working/semisupervised_output/best_semisupervised_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import scipy.ndimage as ndi\n",
    "# You may need to install PraNet if it's a custom or external library\n",
    "# from lib.PraNet_Res2Net import PraNet\n",
    "\n",
    "# =====================================================================================\n",
    "# 0. Model Placeholder \n",
    "# =====================================================================================\n",
    "\n",
    "\n",
    "\n",
    "__all__ = ['Res2Net', 'res2net50_v1b', 'res2net101_v1b', 'res2net50_v1b_26w_4s']\n",
    "\n",
    "model_urls = {\n",
    "    'res2net50_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_v1b_26w_4s-3cf99910.pth',\n",
    "    'res2net101_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net101_v1b_26w_4s-0812c246.pth',\n",
    "}\n",
    "class Bottle2neck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale=4, stype='normal'):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            inplanes: input channel dimensionality\n",
    "            planes: output channel dimensionality\n",
    "            stride: conv stride. Replaces pooling layer.\n",
    "            downsample: None when stride = 1\n",
    "            baseWidth: basic width of conv3x3\n",
    "            scale: number of scale.\n",
    "            type: 'normal': normal set. 'stage': first block of a new stage.\n",
    "        \"\"\"\n",
    "        super(Bottle2neck, self).__init__()\n",
    "\n",
    "        width = int(math.floor(planes * (baseWidth / 64.0)))\n",
    "        self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width * scale)\n",
    "\n",
    "        if scale == 1:\n",
    "            self.nums = 1\n",
    "        else:\n",
    "            self.nums = scale - 1\n",
    "        if stype == 'stage':\n",
    "            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n",
    "        convs = []\n",
    "        bns = []\n",
    "        for i in range(self.nums):\n",
    "            convs.append(nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "            bns.append(nn.BatchNorm2d(width))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList(bns)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stype = stype\n",
    "        self.scale = scale\n",
    "        self.width = width\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0 or self.stype == 'stage':\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.relu(self.bns[i](sp))\n",
    "            if i == 0:\n",
    "                out = sp\n",
    "            else:\n",
    "                out = torch.cat((out, sp), 1)\n",
    "        if self.scale != 1 and self.stype == 'normal':\n",
    "            out = torch.cat((out, spx[self.nums]), 1)\n",
    "        elif self.scale != 1 and self.stype == 'stage':\n",
    "            out = torch.cat((out, self.pool(spx[self.nums])), 1)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Res2Net(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, baseWidth=26, scale=4, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(Res2Net, self).__init__()\n",
    "        self.baseWidth = baseWidth\n",
    "        self.scale = scale\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=stride, stride=stride,\n",
    "                             ceil_mode=True, count_include_pad=False),\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                            stype='stage', baseWidth=self.baseWidth, scale=self.scale))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, baseWidth=self.baseWidth, scale=self.scale))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def res2net50_v1b(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a Res2Net-50_v1b lib.\n",
    "    Res2Net-50 refers to the Res2Net-50_v1b_26w_4s.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s']))\n",
    "    return model\n",
    "def res2net101_v1b(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def res2net50_v1b_26w_4s(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n",
    "    if pretrained:\n",
    "#         model_state = torch.load('/content/drive/MyDrive/ROAD TO MICCAI/MoNuSeg/MonuSeg notebooks/previous notebooks/res2net50_v1b_26w_4s-3cf99910.pth')\n",
    "#         model.load_state_dict(model_state)\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def res2net101_v1b_26w_4s(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def res2net152_v1b_26w_4s(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = Res2Net(Bottle2neck, [3, 8, 36, 3], baseWidth=26, scale=4, **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['res2net152_v1b_26w_4s']))\n",
    "    return model\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RFB_modified(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(RFB_modified, self).__init__()\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv2d(in_channel, out_channel, 1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(in_channel, out_channel, 1),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            BasicConv2d(out_channel, out_channel, 3, padding=3, dilation=3)\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channel, out_channel, 1),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 5), padding=(0, 2)),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(5, 1), padding=(2, 0)),\n",
    "            BasicConv2d(out_channel, out_channel, 3, padding=5, dilation=5)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channel, out_channel, 1),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(7, 1), padding=(3, 0)),\n",
    "            BasicConv2d(out_channel, out_channel, 3, padding=7, dilation=7)\n",
    "        )\n",
    "        self.conv_cat = BasicConv2d(4*out_channel, out_channel, 3, padding=1)\n",
    "        self.conv_res = BasicConv2d(in_channel, out_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x_cat = self.conv_cat(torch.cat((x0, x1, x2, x3), 1))\n",
    "\n",
    "        x = self.relu(x_cat + self.conv_res(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class aggregation(nn.Module):\n",
    "    # dense aggregation, it can be replaced by other aggregation previous, such as DSS, amulet, and so on.\n",
    "    # used after MSF\n",
    "    def __init__(self, channel):\n",
    "        super(aggregation, self).__init__()\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_upsample1 = BasicConv2d(channel, channel, 3, padding=1)\n",
    "        self.conv_upsample2 = BasicConv2d(channel, channel, 3, padding=1)\n",
    "        self.conv_upsample3 = BasicConv2d(channel, channel, 3, padding=1)\n",
    "        self.conv_upsample4 = BasicConv2d(channel, channel, 3, padding=1)\n",
    "        self.conv_upsample5 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n",
    "\n",
    "        self.conv_concat2 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n",
    "        self.conv_concat3 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n",
    "        self.conv4 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(3*channel, 1, 1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1_1 = x1\n",
    "        x2_1 = self.conv_upsample1(self.upsample(x1)) * x2\n",
    "        x3_1 = self.conv_upsample2(self.upsample(self.upsample(x1))) \\\n",
    "               * self.conv_upsample3(self.upsample(x2)) * x3\n",
    "\n",
    "        x2_2 = torch.cat((x2_1, self.conv_upsample4(self.upsample(x1_1))), 1)\n",
    "        x2_2 = self.conv_concat2(x2_2)\n",
    "\n",
    "        x3_2 = torch.cat((x3_1, self.conv_upsample5(self.upsample(x2_2))), 1)\n",
    "        x3_2 = self.conv_concat3(x3_2)\n",
    "\n",
    "        x = self.conv4(x3_2)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PraNet(nn.Module):\n",
    "    # res2net based encoder decoder\n",
    "    def __init__(self, channel=32,num_classes=1):\n",
    "        super(PraNet, self).__init__()\n",
    "        # ---- ResNet Backbone ----\n",
    "        self.resnet = res2net50_v1b_26w_4s(pretrained=True)\n",
    "        # ---- Receptive Field Block like module ----\n",
    "        self.rfb2_1 = RFB_modified(512, channel)\n",
    "        self.rfb3_1 = RFB_modified(1024, channel)\n",
    "        self.rfb4_1 = RFB_modified(2048, channel)\n",
    "        # ---- Partial Decoder ----\n",
    "        self.agg1 = aggregation(channel)\n",
    "        # ---- reverse attention branch 4 ----\n",
    "        self.ra4_conv1 = BasicConv2d(2048, 256, kernel_size=1)\n",
    "        self.ra4_conv2 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n",
    "        self.ra4_conv3 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n",
    "        self.ra4_conv4 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n",
    "        self.ra4_conv5 = BasicConv2d(256, 1, kernel_size=1)\n",
    "        # ---- reverse attention branch 3 ----\n",
    "        self.ra3_conv1 = BasicConv2d(1024, 64, kernel_size=1)\n",
    "        self.ra3_conv2 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.ra3_conv3 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.ra3_conv4 = BasicConv2d(64, 1, kernel_size=3, padding=1)\n",
    "        # ---- reverse attention branch 2 ----\n",
    "        self.ra2_conv1 = BasicConv2d(512, 64, kernel_size=1)\n",
    "        self.ra2_conv2 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.ra2_conv3 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.ra2_conv4 = BasicConv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)      # bs, 64, 88, 88\n",
    "        # ---- low-level features ----\n",
    "        x1 = self.resnet.layer1(x)      # bs, 256, 88, 88\n",
    "        x2 = self.resnet.layer2(x1)     # bs, 512, 44, 44\n",
    "\n",
    "        x3 = self.resnet.layer3(x2)     # bs, 1024, 22, 22\n",
    "        x4 = self.resnet.layer4(x3)     # bs, 2048, 11, 11\n",
    "        x2_rfb = self.rfb2_1(x2)        # channel -> 32\n",
    "        x3_rfb = self.rfb3_1(x3)        # channel -> 32\n",
    "        x4_rfb = self.rfb4_1(x4)        # channel -> 32\n",
    "\n",
    "        ra5_feat = self.agg1(x4_rfb, x3_rfb, x2_rfb)\n",
    "        lateral_map_5 = F.interpolate(ra5_feat, scale_factor=8, mode='bilinear')    # NOTES: Sup-1 (bs, 1, 44, 44) -> (bs, 1, 352, 352)\n",
    "\n",
    "        # ---- reverse attention branch_4 ----\n",
    "        crop_4 = F.interpolate(ra5_feat, scale_factor=0.25, mode='bilinear')\n",
    "        x = -1*(torch.sigmoid(crop_4)) + 1\n",
    "        x = x.expand(-1, 2048, -1, -1).mul(x4)\n",
    "        x = self.ra4_conv1(x)\n",
    "        x = F.relu(self.ra4_conv2(x))\n",
    "        x = F.relu(self.ra4_conv3(x))\n",
    "        x = F.relu(self.ra4_conv4(x))\n",
    "        ra4_feat = self.ra4_conv5(x)\n",
    "        x = ra4_feat + crop_4\n",
    "        lateral_map_4 = F.interpolate(x, scale_factor=32, mode='bilinear')  # NOTES: Sup-2 (bs, 1, 11, 11) -> (bs, 1, 352, 352)\n",
    "\n",
    "        # ---- reverse attention branch_3 ----\n",
    "        crop_3 = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "        x = -1*(torch.sigmoid(crop_3)) + 1\n",
    "        x = x.expand(-1, 1024, -1, -1).mul(x3)\n",
    "        x = self.ra3_conv1(x)\n",
    "        x = F.relu(self.ra3_conv2(x))\n",
    "        x = F.relu(self.ra3_conv3(x))\n",
    "        ra3_feat = self.ra3_conv4(x)\n",
    "        x = ra3_feat + crop_3\n",
    "        lateral_map_3 = F.interpolate(x, scale_factor=16, mode='bilinear')  # NOTES: Sup-3 (bs, 1, 22, 22) -> (bs, 1, 352, 352)\n",
    "\n",
    "        # ---- reverse attention branch_2 ----\n",
    "        crop_2 = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "        x = -1*(torch.sigmoid(crop_2)) + 1\n",
    "        x = x.expand(-1, 512, -1, -1).mul(x2)\n",
    "        x = self.ra2_conv1(x)\n",
    "        x = F.relu(self.ra2_conv2(x))\n",
    "        x = F.relu(self.ra2_conv3(x))\n",
    "        ra2_feat = self.ra2_conv4(x)\n",
    "        x = ra2_feat + crop_2\n",
    "        lateral_map_2 = F.interpolate(x, scale_factor=8, mode='bilinear')   # NOTES: Sup-4 (bs, 1, 44, 44) -> (bs, 1, 352, 352)\n",
    "\n",
    "        # return lateral_map_2,lateral_map_3, lateral_map_4, lateral_map_5\n",
    "        return lateral_map_2\n",
    "def test():\n",
    "    print('testing the code')\n",
    "    x = torch.randn((2, 3, 224,224)) # batch, channel, H, W    \n",
    "    model = PraNet(num_classes=6)\n",
    "    # print(model)\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    pytorch_total__trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('total number of parameters: {}'.format(pytorch_total_params))\n",
    "    print('total number of trainable parameters: {}'.format(pytorch_total__trainable_params))\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    with torch.no_grad():\n",
    "      preds3 = model(x.cuda())\n",
    "    print(f'Input shape : {x.shape}')\n",
    "    print(f'Output shape : {preds3.shape}')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "\n",
    "# =====================================================================================\n",
    "# 1. Configuration\n",
    "# =====================================================================================\n",
    "\n",
    "# --- Core Settings ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMAGE_SIZE = 384\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE_S1 = 1e-4\n",
    "LEARNING_RATE_S2 = 5e-5 # Lower learning rate for fine-tuning\n",
    "NUM_EPOCHS_STAGE_1 = 1\n",
    "NUM_EPOCHS_STAGE_2 = 1\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "VALIDATION_SPLIT = 0.2\n",
    "MIN_LESION_AREA = 50 # Post-processing threshold\n",
    "\n",
    "# --- Path Settings ---\n",
    "# Note: Replace with your actual data paths\n",
    "ORIGINAL_DATA_DIR = \"/kaggle/input/picai-processed/with_mask/kaggle/working/picai_processed_resampled\"\n",
    "UNLABELED_DATA_DIR = \"/kaggle/input/picai-processed/no_mask/kaggle/working/picai_processed_incomplete_cases\"\n",
    "OUTPUT_DIR = \"/kaggle/working/semisupervised_output\"\n",
    "\n",
    "# --- Derived Paths ---\n",
    "PSEUDO_MASK_DIR = os.path.join(OUTPUT_DIR, \"pseudo_masks\")\n",
    "STAGE_1_MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, \"best_supervised_model.pth\")\n",
    "FINAL_MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, \"best_semisupervised_model.pth\")\n",
    "\n",
    "os.makedirs(PSEUDO_MASK_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# 2. Model, Loss, and Data Handling\n",
    "# =====================================================================================\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs are logits, targets are binary masks\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "        inputs_prob = torch.sigmoid(inputs)\n",
    "        intersection = (inputs_prob * targets).sum()\n",
    "        dice_score = (2. * intersection + self.smooth) / (inputs_prob.sum() + targets.sum() + self.smooth)\n",
    "        return bce_loss + (1 - dice_score)\n",
    "\n",
    "\n",
    "class SemiSupPiCaiDataset(Dataset):\n",
    "    def __init__(self, base_dir, sample_ids, mask_dir, is_validation=False):\n",
    "        self.base_dir = base_dir\n",
    "        self.sample_ids = sample_ids\n",
    "        self.mask_dir = mask_dir\n",
    "        self.slice_infos = []\n",
    "\n",
    "        if not self.sample_ids: return\n",
    "\n",
    "        # Determine slice axis from the first sample\n",
    "        first_mask_path = os.path.join(mask_dir, f'{self.sample_ids[0]}.npy')\n",
    "\n",
    "        # Check if the first mask file exists before trying to load it\n",
    "        if not os.path.exists(first_mask_path):\n",
    "            print(f\"Warning: First mask file not found at {first_mask_path}. Cannot initialize dataset.\")\n",
    "            # Handle the case where pseudo-masks might not have been generated for the first ID\n",
    "            # A more robust approach might be to find the first available mask\n",
    "            for s_id in self.sample_ids:\n",
    "                potential_path = os.path.join(mask_dir, f'{s_id}.npy')\n",
    "                if os.path.exists(potential_path):\n",
    "                    first_mask_path = potential_path\n",
    "                    print(f\"Found an existing mask instead: {first_mask_path}\")\n",
    "                    break\n",
    "            else: # No masks found at all\n",
    "                print(\"Error: No mask files found in the specified directory. Aborting dataset creation.\")\n",
    "                return\n",
    "\n",
    "        data_shape = np.load(first_mask_path).shape\n",
    "        self.slice_axis = np.argmin(data_shape)\n",
    "\n",
    "        desc = \"Finding validation slices\" if is_validation else \"Finding training slices\"\n",
    "        for sample_idx, sample_id in enumerate(tqdm(self.sample_ids, desc=desc)):\n",
    "            mask_path = os.path.join(self.mask_dir, f'{sample_id}.npy')\n",
    "            if not os.path.exists(mask_path):\n",
    "                continue # Skip if a pseudo-mask failed to generate for this ID\n",
    "\n",
    "            mask_3d = np.load(mask_path)\n",
    "            num_slices = mask_3d.shape[self.slice_axis]\n",
    "\n",
    "            # For final evaluation, we need all slices. Otherwise, balance them.\n",
    "            if is_validation:\n",
    "                for slice_idx in range(num_slices):\n",
    "                    self.slice_infos.append((sample_idx, slice_idx))\n",
    "            else:\n",
    "                pos_slices = [i for i in range(num_slices) if np.sum(np.take(mask_3d, i, self.slice_axis)) > 0]\n",
    "                neg_slices = [i for i in range(num_slices) if np.sum(np.take(mask_3d, i, self.slice_axis)) == 0]\n",
    "                num_pos = len(pos_slices)\n",
    "\n",
    "                for slice_idx in pos_slices: self.slice_infos.append((sample_idx, slice_idx))\n",
    "\n",
    "                if neg_slices: # Add an equal number of negative slices\n",
    "                    neg_samples = random.sample(neg_slices, min(num_pos, len(neg_slices)))\n",
    "                    for slice_idx in neg_samples: self.slice_infos.append((sample_idx, slice_idx))\n",
    "\n",
    "        random.shuffle(self.slice_infos)\n",
    "        print(f\"Loaded {len(self.slice_infos)} 2D slices for this dataset partition.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_infos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, slice_idx = self.slice_infos[idx]\n",
    "        sample_id = self.sample_ids[sample_idx]\n",
    "\n",
    "        modalities = [np.load(os.path.join(self.base_dir, m, f'{sample_id}.npy')) for m in ['t2w', 'adc', 'hbv']]\n",
    "        img_3d = np.stack(modalities, axis=-1)\n",
    "\n",
    "        mask_path = os.path.join(self.mask_dir, f'{sample_id}.npy')\n",
    "        mask_3d = np.load(mask_path)\n",
    "\n",
    "        image_slice = np.take(img_3d, slice_idx, axis=self.slice_axis)\n",
    "        mask_slice = np.take(mask_3d, slice_idx, axis=self.slice_axis)\n",
    "\n",
    "        # Ensure mask is binary\n",
    "        mask_slice = (mask_slice > 0).astype(np.float32)\n",
    "\n",
    "        return image_slice, mask_slice\n",
    "\n",
    "\n",
    "class AugmentationWrapper(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_np, mask_np = self.dataset[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image_np.astype(np.float32), mask=mask_np)\n",
    "            image_np, mask_np = augmented['image'], augmented['mask']\n",
    "\n",
    "        # CRITICAL: Apply percentile normalization AFTER augmentations but BEFORE ToTensor\n",
    "        for i in range(image_np.shape[2]):\n",
    "            channel = image_np[:, :, i]\n",
    "            non_zero = channel[channel > 1e-6]\n",
    "            if non_zero.size > 0:\n",
    "                p1, p99 = np.percentile(non_zero, 1), np.percentile(non_zero, 99)\n",
    "                channel = np.clip(channel, p1, p99)\n",
    "            min_val, max_val = channel.min(), channel.max()\n",
    "            image_np[:, :, i] = (channel - min_val) / (max_val - min_val) if max_val > min_val else 0\n",
    "\n",
    "        image = torch.from_numpy(image_np.transpose(2, 0, 1)).float()\n",
    "        mask = torch.from_numpy(mask_np).unsqueeze(0).float() # Add channel dim\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# 3. Training and Mid-Training Validation\n",
    "# =====================================================================================\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    loop = tqdm(loader, desc=\"Training\")\n",
    "    for data, targets in loop:\n",
    "        data, targets = data.to(DEVICE), targets.to(DEVICE)\n",
    "        with torch.amp.autocast(device_type=DEVICE.split(':')[0], enabled=(DEVICE==\"cuda\")):\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    \"\"\" Fast check on a balanced validation set for model selection. \"\"\"\n",
    "    model.eval()\n",
    "    dice_num, dice_den = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.amp.autocast(device_type=device.split(':')[0], enabled=(device==\"cuda\")):\n",
    "                preds = torch.sigmoid(model(x))\n",
    "            preds_binary = (preds > 0.5).float()\n",
    "            dice_num += 2 * (preds_binary * y).sum()\n",
    "            dice_den += preds_binary.sum() + y.sum()\n",
    "    model.train()\n",
    "    dice_score = (dice_num + 1e-8) / (dice_den + 1e-8)\n",
    "    return float(dice_score)\n",
    "\n",
    "# =====================================================================================\n",
    "# 4. Final Evaluation (2D and 3D)\n",
    "# =====================================================================================\n",
    "\n",
    "def calculate_final_2d_metrics(val_loader_all_slices, model, device):\n",
    "    print(\"\\nCalculating Final 2D Metrics (on all validation slices)...\")\n",
    "    model.eval()\n",
    "    all_preds_flat, all_targets_flat = [], []\n",
    "    total_dice_score, slice_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(val_loader_all_slices, desc=\"2D Evaluation\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.amp.autocast(device_type=device.split(':')[0], enabled=(device==\"cuda\")):\n",
    "                preds_prob = torch.sigmoid(model(x))\n",
    "\n",
    "            for i in range(x.size(0)):\n",
    "                pred_prob_slice = preds_prob[i].squeeze()\n",
    "                target_slice = y[i].squeeze()\n",
    "                pred_binary_slice = (pred_prob_slice > 0.5).float()\n",
    "\n",
    "                # Dice Score\n",
    "                intersection = (pred_binary_slice * target_slice).sum()\n",
    "                union = pred_binary_slice.sum() + target_slice.sum()\n",
    "                total_dice_score += (2. * intersection + 1e-8) / (union + 1e-8)\n",
    "                slice_count += 1\n",
    "\n",
    "                # For AUROC/AP\n",
    "                all_preds_flat.append(pred_prob_slice.view(-1).cpu().numpy())\n",
    "                all_targets_flat.append(target_slice.view(-1).cpu().numpy().astype(bool))\n",
    "\n",
    "    # Aggregate and compute final metrics\n",
    "    avg_dice = total_dice_score / slice_count if slice_count > 0 else 0\n",
    "\n",
    "    y_true_all = np.concatenate(all_targets_flat)\n",
    "    y_pred_all = np.concatenate(all_preds_flat)\n",
    "\n",
    "    auroc = roc_auc_score(y_true_all, y_pred_all) if len(np.unique(y_true_all)) > 1 else 0\n",
    "    ap = average_precision_score(y_true_all, y_pred_all) if len(np.unique(y_true_all)) > 1 else 0\n",
    "\n",
    "    return {\"2d_dice\": float(avg_dice), \"2d_auroc\": auroc, \"2d_ap\": ap}\n",
    "\n",
    "\n",
    "def calculate_final_3d_metrics(val_ids, base_dir, model, device):\n",
    "    print(\"\\nCalculating Final 3D Metrics (on validation volumes)...\")\n",
    "    model.eval()\n",
    "\n",
    "    # Define the exact same transform used for training/validation\n",
    "    eval_transform = AugmentationWrapper(dataset=None, transform=A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE)]))\n",
    "\n",
    "    patient_metrics = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for patient_id in tqdm(val_ids, desc=\"3D Evaluation\"):\n",
    "            try:\n",
    "                # Load ground truth volume\n",
    "                gt_path = os.path.join(base_dir, 'mask', f'{patient_id}.npy')\n",
    "                if not os.path.exists(gt_path): continue\n",
    "                gt_volume = np.load(gt_path)\n",
    "                gt_volume_binary = (gt_volume > 0).astype(bool)\n",
    "\n",
    "                # Determine slice axis and prepare for reconstruction\n",
    "                slice_axis = np.argmin(gt_volume.shape)\n",
    "                pred_volume_prob = np.zeros_like(gt_volume, dtype=float)\n",
    "\n",
    "                # Iterate over each slice for the patient\n",
    "                for slice_idx in range(gt_volume.shape[slice_axis]):\n",
    "                    # Manually create the input slice\n",
    "                    modalities = [np.load(os.path.join(base_dir, m, f'{patient_id}.npy')) for m in ['t2w', 'adc', 'hbv']]\n",
    "                    image_slice_np = np.stack([np.take(vol, slice_idx, axis=slice_axis) for vol in modalities], axis=-1)\n",
    "\n",
    "                    # Apply the same augmentation/normalization\n",
    "                    eval_transform.dataset = [(image_slice_np, np.zeros_like(image_slice_np[..., 0]))]\n",
    "                    image_tensor, _ = eval_transform[0]\n",
    "                    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "                    # Get model prediction\n",
    "                    with torch.amp.autocast(device_type=device.split(':')[0], enabled=(device==\"cuda\")):\n",
    "                        pred_prob = torch.sigmoid(model(image_tensor)).squeeze()\n",
    "\n",
    "                    # Resize back to original dimensions and place in volume\n",
    "                    original_dims = (gt_volume.shape[1], gt_volume.shape[2]) # H, W\n",
    "                    pred_prob_resized = F.interpolate(pred_prob.unsqueeze(0).unsqueeze(0), size=original_dims, mode='bilinear', align_corners=False).squeeze().cpu().numpy()\n",
    "\n",
    "                    # Place the resized slice back into the 3D volume using direct assignment\n",
    "                    slicer = [slice(None)] * pred_volume_prob.ndim\n",
    "                    slicer[slice_axis] = slice_idx\n",
    "                    pred_volume_prob[tuple(slicer)] = pred_prob_resized\n",
    "\n",
    "                # Calculate metrics for the entire volume\n",
    "                pred_volume_binary = (pred_volume_prob > 0.5)\n",
    "\n",
    "                # 3D Dice\n",
    "                intersection = (pred_volume_binary & gt_volume_binary).sum()\n",
    "                union = pred_volume_binary.sum() + gt_volume_binary.sum()\n",
    "                dice_3d = (2. * intersection + 1e-8) / (union + 1e-8)\n",
    "\n",
    "                # 3D AUROC & AP\n",
    "                gt_flat = gt_volume_binary.flatten()\n",
    "                pred_flat = pred_volume_prob.flatten()\n",
    "\n",
    "                auroc_3d = roc_auc_score(gt_flat, pred_flat) if len(np.unique(gt_flat)) > 1 else 0\n",
    "                ap_3d = average_precision_score(gt_flat, pred_flat) if len(np.unique(gt_flat)) > 1 else 0\n",
    "\n",
    "                patient_metrics.append({\"dice\": dice_3d, \"auroc\": auroc_3d, \"ap\": ap_3d})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process patient {patient_id} for 3D eval. Error: {e}\")\n",
    "\n",
    "    # Average metrics across all patients\n",
    "    avg_dice = np.mean([m['dice'] for m in patient_metrics]) if patient_metrics else 0\n",
    "    avg_auroc = np.mean([m['auroc'] for m in patient_metrics]) if patient_metrics else 0\n",
    "    avg_ap = np.mean([m['ap'] for m in patient_metrics]) if patient_metrics else 0\n",
    "\n",
    "    model.train()\n",
    "    return {\"3d_dice\": avg_dice, \"3d_auroc\": avg_auroc, \"3d_ap\": avg_ap}\n",
    "\n",
    "\n",
    "def generate_pseudo_labels(unlabeled_ids, base_dir, out_dir, model, device):\n",
    "    \"\"\"Slightly simplified version of 3D eval to just save masks.\"\"\"\n",
    "    model.eval()\n",
    "    eval_transform = AugmentationWrapper(dataset=None, transform=A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE)]))\n",
    "    with torch.no_grad():\n",
    "        for patient_id in tqdm(unlabeled_ids, desc=\"Generating Pseudo-Masks\"):\n",
    "            try:\n",
    "                ref_vol_path = os.path.join(base_dir, 't2w', f'{patient_id}.npy')\n",
    "                if not os.path.exists(ref_vol_path): continue\n",
    "                ref_vol = np.load(ref_vol_path)\n",
    "                slice_axis = np.argmin(ref_vol.shape)\n",
    "                pred_volume = np.zeros_like(ref_vol, dtype=np.uint8)\n",
    "\n",
    "                for slice_idx in range(ref_vol.shape[slice_axis]):\n",
    "                    modalities = [np.load(os.path.join(base_dir, m, f'{patient_id}.npy')) for m in ['t2w', 'adc', 'hbv']]\n",
    "                    image_slice_np = np.stack([np.take(vol, slice_idx, axis=slice_axis) for vol in modalities], axis=-1)\n",
    "\n",
    "                    eval_transform.dataset = [(image_slice_np, np.zeros_like(image_slice_np[...,0]))]\n",
    "                    image_tensor, _ = eval_transform[0]\n",
    "                    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "                    with torch.amp.autocast(device_type=device.split(':')[0], enabled=(device==\"cuda\")):\n",
    "                        pred_prob = torch.sigmoid(model(image_tensor)).squeeze()\n",
    "\n",
    "                    pred_binary = (pred_prob > 0.5).cpu().numpy().astype(np.uint8)\n",
    "                    original_dims = (ref_vol.shape[1], ref_vol.shape[2]) # H, W\n",
    "\n",
    "                    # ✅ CORRECTED: Instantiate Resize and then apply it.\n",
    "                    resizer = A.Resize(height=original_dims[0], width=original_dims[1], interpolation=0)\n",
    "                    pred_resized = resizer(image=pred_binary)['image']\n",
    "\n",
    "                    # Post-process to remove small artifacts\n",
    "                    pred_processed = remove_small_lesions(pred_resized, MIN_LESION_AREA)\n",
    "\n",
    "                    # Place the processed slice back into the 3D volume using direct assignment\n",
    "                    slicer = [slice(None)] * pred_volume.ndim\n",
    "                    slicer[slice_axis] = slice_idx\n",
    "                    pred_volume[tuple(slicer)] = pred_processed\n",
    "\n",
    "                np.save(os.path.join(out_dir, f\"{patient_id}.npy\"), pred_volume)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not pseudo-label patient {patient_id}. Error: {e}\")\n",
    "    model.train()\n",
    "\n",
    "def remove_small_lesions(mask_np, min_size):\n",
    "    \"\"\"Binary version of remove_small_lesions.\"\"\"\n",
    "    labeled_array, num_features = ndi.label(mask_np)\n",
    "    if num_features == 0:\n",
    "        return mask_np\n",
    "\n",
    "    component_sizes = np.bincount(labeled_array.ravel())\n",
    "    large_enough = component_sizes > min_size\n",
    "    large_enough[0] = False # Background is not a lesion\n",
    "\n",
    "    return large_enough[labeled_array].astype(np.uint8)\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# 5. Main Execution Block\n",
    "# =====================================================================================\n",
    "def main():\n",
    "    # --- Data Setup ---\n",
    "    all_original_ids = sorted([f.replace('.npy', '') for f in os.listdir(os.path.join(ORIGINAL_DATA_DIR, 'mask')) if f.endswith('.npy')])\n",
    "    random.seed(42); random.shuffle(all_original_ids)\n",
    "    split_idx = int(len(all_original_ids) * (1 - VALIDATION_SPLIT))\n",
    "    original_train_ids, val_ids = all_original_ids[:split_idx], all_original_ids[split_idx:]\n",
    "    print(f\"Labeled Data Split: {len(original_train_ids)} training, {len(val_ids)} validation patients.\")\n",
    "\n",
    "    # --- Transforms ---\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE), A.Rotate(limit=15, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.1),\n",
    "        A.RandomBrightnessContrast(p=0.3), A.GaussNoise(p=0.2)\n",
    "    ])\n",
    "    val_transform = A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE)])\n",
    "\n",
    "    # --- STAGE 1: Supervised Training ---\n",
    "    print(\"\\n--- STAGE 1: Starting Supervised Training ---\")\n",
    "    model = PraNet().to(DEVICE)\n",
    "    loss_fn = DiceBCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_S1)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "    # Create datasets for Stage 1\n",
    "    train_base_s1 = SemiSupPiCaiDataset(ORIGINAL_DATA_DIR, original_train_ids, mask_dir=os.path.join(ORIGINAL_DATA_DIR, 'mask'))\n",
    "    val_base_s1 = SemiSupPiCaiDataset(ORIGINAL_DATA_DIR, val_ids, mask_dir=os.path.join(ORIGINAL_DATA_DIR, 'mask'), is_validation=True) # Use full validation set here\n",
    "    train_dataset_s1 = AugmentationWrapper(train_base_s1, transform=train_transform)\n",
    "    val_dataset_s1 = AugmentationWrapper(val_base_s1, transform=val_transform)\n",
    "\n",
    "    train_loader_s1 = DataLoader(train_dataset_s1, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "    # For model selection, use a balanced loader from the validation set to be consistent\n",
    "    val_base_s1_balanced = SemiSupPiCaiDataset(ORIGINAL_DATA_DIR, val_ids, mask_dir=os.path.join(ORIGINAL_DATA_DIR, 'mask'), is_validation=False)\n",
    "    val_dataset_s1_balanced = AugmentationWrapper(val_base_s1_balanced, transform=val_transform)\n",
    "    val_loader_s1_balanced = DataLoader(val_dataset_s1_balanced, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "    best_val_dice = -1.0\n",
    "    for epoch in range(NUM_EPOCHS_STAGE_1):\n",
    "        print(f\"\\n--- Stage 1, Epoch {epoch+1}/{NUM_EPOCHS_STAGE_1} ---\")\n",
    "        train_fn(train_loader_s1, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "        current_dice = check_accuracy(val_loader_s1_balanced, model, device=DEVICE)\n",
    "        print(f\"Validation Dice (Balanced Set): {current_dice:.4f}\")\n",
    "\n",
    "        if current_dice > best_val_dice:\n",
    "            best_val_dice = current_dice\n",
    "            torch.save(model.state_dict(), STAGE_1_MODEL_SAVE_PATH)\n",
    "            print(f\"==> New best Stage 1 model saved with Lesion Dice: {best_val_dice:.4f}\")\n",
    "\n",
    "    # --- Pseudo-Label Generation ---\n",
    "    print(\"\\n--- Generating Pseudo-Labels for Stage 2 ---\")\n",
    "    model.load_state_dict(torch.load(STAGE_1_MODEL_SAVE_PATH)) # Load best model\n",
    "    unlabeled_ids = sorted([f.replace('.npy', '') for f in os.listdir(os.path.join(UNLABELED_DATA_DIR, 't2w'))])\n",
    "\n",
    "    generate_pseudo_labels(unlabeled_ids, UNLABELED_DATA_DIR, PSEUDO_MASK_DIR, model, DEVICE)\n",
    "\n",
    "\n",
    "    # --- STAGE 2: Semi-Supervised Training ---\n",
    "    print(\"\\n--- STAGE 2: Starting Semi-Supervised Training ---\")\n",
    "    # Load best model from Stage 1 to fine-tune\n",
    "    model.load_state_dict(torch.load(STAGE_1_MODEL_SAVE_PATH))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_S2) # Reset optimizer with lower LR\n",
    "\n",
    "    # Create dataset for pseudo-labeled data\n",
    "    pseudo_train_base = SemiSupPiCaiDataset(UNLABELED_DATA_DIR, unlabeled_ids, mask_dir=PSEUDO_MASK_DIR)\n",
    "\n",
    "    # Combine original labeled data and pseudo-labeled data\n",
    "    if len(pseudo_train_base) > 0:\n",
    "        combined_train_dataset = ConcatDataset([train_base_s1, pseudo_train_base])\n",
    "        print(f\"Combining {len(train_base_s1)} labeled slices with {len(pseudo_train_base)} pseudo-labeled slices.\")\n",
    "    else:\n",
    "        print(\"No pseudo-labeled slices were generated. Proceeding with only labeled data for Stage 2.\")\n",
    "        combined_train_dataset = train_base_s1\n",
    "\n",
    "    train_dataset_s2 = AugmentationWrapper(combined_train_dataset, transform=train_transform)\n",
    "    train_loader_s2 = DataLoader(train_dataset_s2, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS_STAGE_2):\n",
    "        print(f\"\\n--- Stage 2, Epoch {epoch+1}/{NUM_EPOCHS_STAGE_2} ---\")\n",
    "        train_fn(train_loader_s2, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "        current_dice = check_accuracy(val_loader_s1_balanced, model, device=DEVICE)\n",
    "        print(f\"Validation Dice (Balanced Set): {current_dice:.4f}\")\n",
    "\n",
    "        if current_dice > best_val_dice:\n",
    "            best_val_dice = current_dice\n",
    "            torch.save(model.state_dict(), FINAL_MODEL_SAVE_PATH)\n",
    "            print(f\"==> New best Stage 2 model saved with Lesion Dice: {best_val_dice:.4f}\")\n",
    "\n",
    "    # --- Final Comprehensive Evaluation ---\n",
    "    print(\"\\n--- FINAL EVALUATION ---\")\n",
    "    # Load the absolute best model (could be from stage 1 or 2)\n",
    "    final_model_path = FINAL_MODEL_SAVE_PATH if os.path.exists(FINAL_MODEL_SAVE_PATH) else STAGE_1_MODEL_SAVE_PATH\n",
    "    print(f\"Loading best model from: {final_model_path}\")\n",
    "    model.load_state_dict(torch.load(final_model_path))\n",
    "\n",
    "    # Create a validation loader with ALL slices for realistic metrics\n",
    "    val_loader_all_slices = DataLoader(val_dataset_s1, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "    metrics_2d = calculate_final_2d_metrics(val_loader_all_slices, model, DEVICE)\n",
    "    metrics_3d = calculate_final_3d_metrics(val_ids, ORIGINAL_DATA_DIR, model, DEVICE)\n",
    "\n",
    "    print(\"\\n--- Final 2D Slice-Level Metrics ---\")\n",
    "    print(f\"  Dice Score: {metrics_2d['2d_dice']:.4f}\")\n",
    "    print(f\"  AUROC:      {metrics_2d['2d_auroc']:.4f}\")\n",
    "    print(f\"  AP:         {metrics_2d['2d_ap']:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Final 3D Patient-Level Metrics ---\")\n",
    "    print(f\"  Dice Score: {metrics_3d['3d_dice']:.4f}\")\n",
    "    print(f\"  AUROC:      {metrics_3d['3d_auroc']:.4f}\")\n",
    "    print(f\"  AP:         {metrics_3d['3d_ap']:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining finished. Best model at: {final_model_path}\")\n",
    "    # shutil.rmtree(PSEUDO_MASK_DIR)\n",
    "    # print(\"Cleanup complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Deprecation warnings for torch.amp are informational. The corrected code\n",
    "    # uses the newer `torch.amp.autocast` and `GradScaler` syntax where appropriate.\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_2d_slice_metrics(loader, model, device=\"cuda\", threshold=0.5):\n",
    "    \"\"\"Calculates 2D metrics (Dice, AUROC, AP or PR-AUC) using the validation loader.\"\"\"\n",
    "    model.eval()\n",
    "    all_targets_flat = []\n",
    "    all_preds_prob_flat = []\n",
    "    total_dice_score = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Calculating 2D Slice Metrics\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.amp.autocast(device_type=str(device)):\n",
    "                preds = model(x)\n",
    "                preds_prob = torch.sigmoid(preds)\n",
    "\n",
    "            preds_binary = (preds_prob > threshold).float()\n",
    "            intersection = (preds_binary * y).sum()\n",
    "            dice_score = (2. * intersection) / (preds_binary.sum() + y.sum() + 1e-6)\n",
    "            total_dice_score += dice_score.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            all_targets_flat.append(y.cpu().numpy().flatten())\n",
    "            all_preds_prob_flat.append(preds_prob.cpu().numpy().flatten())\n",
    "\n",
    "    avg_dice = total_dice_score / num_batches if num_batches > 0 else 0\n",
    "    y_true = np.concatenate(all_targets_flat)\n",
    "    y_pred = np.concatenate(all_preds_prob_flat)\n",
    "\n",
    "    y_true = (y_true > 0.5).astype(np.uint8)\n",
    "\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        auroc = roc_auc_score(y_true, y_pred)\n",
    "        pr_auc = average_precision_score(y_true, y_pred)\n",
    "    else:\n",
    "        auroc = 0.0\n",
    "        pr_auc = 0.0\n",
    "\n",
    "    return {\"dice\": avg_dice, \"auroc\": auroc, \"pr_auc\": pr_auc}\n",
    "def calculate_3d_volume_metrics(model, val_ids, base_dir, val_transform, device, threshold=0.5):\n",
    "    \"\"\"Calculates 3D Dice, AUROC, and PR-AUC by iterating through each patient volume.\"\"\"\n",
    "    print(\"\\nStarting 3D volume evaluation...\")\n",
    "    model.eval()\n",
    "    patient_dice_scores = []\n",
    "    # Lists to store all flattened GT and predictions for overall metrics\n",
    "    all_patient_gt_flat = []\n",
    "    all_patient_pred_flat = []\n",
    "\n",
    "    def normalize_slice(image_np):\n",
    "        for i in range(image_np.shape[2]):\n",
    "            channel = image_np[:, :, i]\n",
    "            non_zero_pixels = channel[channel > 1e-6]\n",
    "            if non_zero_pixels.size > 0:\n",
    "                p1, p99 = np.percentile(non_zero_pixels, 1), np.percentile(non_zero_pixels, 99)\n",
    "                channel = np.clip(channel, p1, p99)\n",
    "            min_val, max_val = channel.min(), channel.max()\n",
    "            if max_val > min_val:\n",
    "                image_np[:, :, i] = (channel - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                image_np[:, :, i] = np.zeros_like(channel)\n",
    "        return image_np\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for patient_id in tqdm(val_ids, desc=\"Calculating 3D Volume Metrics\"):\n",
    "            modalities = []\n",
    "            for modality in ['t2w', 'adc', 'hbv']:\n",
    "                img_path = os.path.join(base_dir, modality, f'{patient_id}.npy')\n",
    "                modalities.append(np.load(img_path))\n",
    "            \n",
    "            img_3d = np.stack(modalities, axis=-1)\n",
    "            mask_3d_gt = np.load(os.path.join(base_dir, 'mask', f'{patient_id}.npy'))\n",
    "\n",
    "            slice_axis = np.argmin(mask_3d_gt.shape)\n",
    "            num_slices = mask_3d_gt.shape[slice_axis]\n",
    "            \n",
    "            all_dims = list(range(mask_3d_gt.ndim))\n",
    "            all_dims.remove(slice_axis)\n",
    "            original_h, original_w = mask_3d_gt.shape[all_dims[0]], mask_3d_gt.shape[all_dims[1]]\n",
    "            \n",
    "            pred_slices_list = []\n",
    "\n",
    "            for slice_idx in range(num_slices):\n",
    "                slicer = [slice(None)] * 3\n",
    "                slicer[slice_axis] = slice_idx\n",
    "                image_slice_np = img_3d[tuple(slicer)]\n",
    "\n",
    "                augmented = val_transform(image=image_slice_np.astype(np.float32))\n",
    "                image_slice_np_aug = augmented['image']\n",
    "                \n",
    "                image_slice_np_norm = normalize_slice(image_slice_np_aug)\n",
    "                image_tensor = torch.from_numpy(image_slice_np_norm.transpose(2, 0, 1)).float().unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.amp.autocast(device_type=str(device)):\n",
    "                    pred_logit = model(image_tensor)\n",
    "                    pred_prob = torch.sigmoid(pred_logit)\n",
    "\n",
    "                pred_prob_resized = F.interpolate(pred_prob, size=(original_h, original_w), mode='bilinear', align_corners=False)\n",
    "                pred_slices_list.append(pred_prob_resized.squeeze().cpu())\n",
    "\n",
    "            pred_3d_volume_prob = torch.stack(pred_slices_list, axis=slice_axis)\n",
    "            \n",
    "            pred_3d_binary = (pred_3d_volume_prob > threshold).float()\n",
    "            \n",
    "            mask_3d_gt_tensor = torch.from_numpy(mask_3d_gt).float()\n",
    "            \n",
    "            intersection = (pred_3d_binary * mask_3d_gt_tensor).sum()\n",
    "            dice_3d = (2. * intersection) / (pred_3d_binary.sum() + mask_3d_gt_tensor.sum() + 1e-6)\n",
    "            patient_dice_scores.append(dice_3d.item())\n",
    "\n",
    "            all_patient_gt_flat.append(mask_3d_gt_tensor.numpy().flatten())\n",
    "            all_patient_pred_flat.append(pred_3d_volume_prob.numpy().flatten())\n",
    "\n",
    "    avg_3d_dice = np.mean(patient_dice_scores) if patient_dice_scores else 0.0\n",
    "\n",
    "    y_true_3d = np.concatenate(all_patient_gt_flat)\n",
    "    y_pred_3d = np.concatenate(all_patient_pred_flat)\n",
    "\n",
    "    y_true_3d = (y_true_3d > 0.5).astype(np.uint8)\n",
    "\n",
    "    if len(np.unique(y_true_3d)) > 1:\n",
    "        auroc_3d = roc_auc_score(y_true_3d, y_pred_3d)\n",
    "        pr_auc_3d = average_precision_score(y_true_3d, y_pred_3d)\n",
    "    else:\n",
    "        print(\"Warning: Only one class present in 3D validation volumes. AUROC and PR-AUC cannot be computed.\")\n",
    "        auroc_3d = 0.0\n",
    "        pr_auc_3d = 0.0\n",
    "\n",
    "    return {\"3d_dice\": avg_3d_dice, \"3d_auroc\": auroc_3d, \"3d_pr_auc\": pr_auc_3d}\n",
    "\n",
    "\n",
    "def main():\n",
    "    val_transform = A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE)])\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    model = PraNet().to(DEVICE)\n",
    "\n",
    "    try:\n",
    "        mask_dir = os.path.join(DATA_DIR, 'mask')\n",
    "        all_sample_ids = sorted([f.replace('.npy', '') for f in os.listdir(mask_dir) if f.endswith('.npy')])\n",
    "        if not all_sample_ids: raise FileNotFoundError\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nERROR: No mask files (.npy) found in {mask_dir}. Check DATA_DIR.\")\n",
    "        return\n",
    "\n",
    "    # Use the same seed and split to ensure the validation set is identical\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_sample_ids)\n",
    "    split_idx = int(len(all_sample_ids) * (1 - VALIDATION_SPLIT))\n",
    "    train_ids, val_ids = all_sample_ids[:split_idx], all_sample_ids[split_idx:]\n",
    "    print(f\"\\nTotal patient volumes: {len(all_sample_ids)}, Training: {len(train_ids)}, Validation: {len(val_ids)}\\n\")\n",
    "\n",
    "    # We only need the validation dataset for evaluation\n",
    "    val_base_dataset = PiCai2DDataset(DATA_DIR, sample_ids=val_ids)\n",
    "    val_dataset = AugmentationWrapper(val_base_dataset, transform=val_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "\n",
    "    print(\"--- Starting Evaluation on Pre-trained Model ---\")\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Model file not found at '{MODEL_SAVE_PATH}'. Cannot evaluate.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the model: {e}\")\n",
    "        return\n",
    "\n",
    "    # 1. Final 2D Slice-based Metrics\n",
    "    final_metrics_2d = calculate_2d_slice_metrics(val_loader, model, device=DEVICE)\n",
    "    print(\"\\n--- Final 2D Slice-Based Metrics ---\")\n",
    "    print(f\"Dice Score: {final_metrics_2d['dice']:.4f}\")\n",
    "    print(f\"AUROC: {final_metrics_2d['auroc']:.4f}\")\n",
    "    print(f\"PR-AUC (Average Precision): {final_metrics_2d['pr_auc']:.4f}\")\n",
    "\n",
    "    # 2. Final 3D Volume-based Metrics\n",
    "    final_metrics_3d = calculate_3d_volume_metrics(model, val_ids, DATA_DIR, val_transform, DEVICE)\n",
    "    print(\"\\n--- Final 3D Volume-Based Metrics ---\")\n",
    "    print(f\"Average Patient Dice Score: {final_metrics_3d['3d_dice']:.4f}\")\n",
    "    print(f\"Overall AUROC: {final_metrics_3d['3d_auroc']:.4f}\")\n",
    "    print(f\"Overall PR-AUC (Average Precision): {final_metrics_3d['3d_pr_auc']:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE)])\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "model = PraNet().to(DEVICE)\n",
    "\n",
    "try:\n",
    "    mask_dir = os.path.join(DATA_DIR, 'mask')\n",
    "    all_sample_ids = sorted([f.replace('.npy', '') for f in os.listdir(mask_dir) if f.endswith('.npy')])\n",
    "    if not all_sample_ids: raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERROR: No mask files (.npy) found in {mask_dir}. Check DATA_DIR.\")\n",
    "    # return\n",
    "\n",
    "MODEL_SAVE_PATH = \"best_unet_model_balanced_eval.pth\"\n",
    "random.seed(42)\n",
    "random.shuffle(all_sample_ids)\n",
    "split_idx = int(len(all_sample_ids) * (1 - VALIDATION_SPLIT))\n",
    "train_ids, val_ids = all_sample_ids[:split_idx], all_sample_ids[split_idx:]\n",
    "print(f\"\\nTotal samples: {len(all_sample_ids)}, Training: {len(train_ids)}, Validation: {len(val_ids)}\\n\")\n",
    "\n",
    "val_base_dataset = PiCai2DDataset(DATA_DIR, sample_ids=val_ids)\n",
    "val_dataset = AugmentationWrapper(val_base_dataset, transform=val_transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "\n",
    "def visualize_all_predictions(model, loader, device):\n",
    "    print(\"\\nLoading best model for visualization...\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "        model.to(device)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Model file not found at '{MODEL_SAVE_PATH}'. Cannot visualize.\")\n",
    "        print(\"Please ensure the model was trained and saved successfully.\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if len(loader.dataset) == 0:\n",
    "        print(\"Validation loader is empty. Cannot visualize predictions.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Generating predictions for all {len(loader.dataset)} validation images...\")\n",
    "    with torch.no_grad():\n",
    "        # Iterate through each batch in the loader\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Visualizing Batches\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            with torch.amp.autocast(device_type=str(device)):\n",
    "                preds = torch.sigmoid(model(images))\n",
    "                preds_binary = (preds > 0.5).float()\n",
    "\n",
    "            num_images_in_batch = len(images)\n",
    "\n",
    "            # Create a flexible subplot grid for the current batch\n",
    "            # Handle the edge case where the batch size is 1\n",
    "            fig, axes = plt.subplots(num_images_in_batch, 3, figsize=(15, num_images_in_batch * 5))\n",
    "            if num_images_in_batch == 1:\n",
    "                axes = np.array([axes])\n",
    "\n",
    "            fig.suptitle(f\"Batch {batch_idx + 1}/{len(loader)}: Predictions vs. Ground Truth\", fontsize=18)\n",
    "\n",
    "            for i in range(num_images_in_batch):\n",
    "                # Column 1: Input Image (T2W channel)\n",
    "                ax = axes[i, 0]\n",
    "                ax.imshow(images[i][0].cpu().numpy(), cmap='gray')\n",
    "                ax.set_title(f\"Input Image (T2W)\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "                # Column 2: Ground Truth Mask\n",
    "                ax = axes[i, 1]\n",
    "                ax.imshow(masks[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "                ax.set_title(\"Ground Truth Mask\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "                # Column 3: Predicted Mask\n",
    "                ax = axes[i, 2]\n",
    "                ax.imshow(preds_binary[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "                ax.set_title(\"Predicted Mask\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.95) # Adjust for suptitle\n",
    "            plt.show()\n",
    "\n",
    "visualize_all_predictions(model, val_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE)])\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "model = PraNet().to(DEVICE)\n",
    "\n",
    "try:\n",
    "    mask_dir = os.path.join(DATA_DIR, 'mask')\n",
    "    all_sample_ids = sorted([f.replace('.npy', '') for f in os.listdir(mask_dir) if f.endswith('.npy')])\n",
    "    if not all_sample_ids: raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERROR: No mask files (.npy) found in {mask_dir}. Check DATA_DIR.\")\n",
    "    # return\n",
    "\n",
    "MODEL_SAVE_PATH = \"best_unet_model_balanced_eval.pth\"\n",
    "random.seed(42)\n",
    "random.shuffle(all_sample_ids)\n",
    "split_idx = int(len(all_sample_ids) * (1 - VALIDATION_SPLIT))\n",
    "train_ids, val_ids = all_sample_ids[:split_idx], all_sample_ids[split_idx:]\n",
    "print(f\"\\nTotal samples: {len(all_sample_ids)}, Training: {len(train_ids)}, Validation: {len(val_ids)}\\n\")\n",
    "\n",
    "val_base_dataset = PiCai2DDataset(DATA_DIR, sample_ids=val_ids)\n",
    "val_dataset = AugmentationWrapper(val_base_dataset, transform=val_transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "\n",
    "def visualize_3d_with_image_overlay(\n",
    "    image_3d, \n",
    "    ground_truth_mask_3d, \n",
    "    predicted_mask_3d, \n",
    "    sample_id=\"\", \n",
    "    downsample_factor=0.25,\n",
    "    show_gt=True,\n",
    "    show_pred=True\n",
    "):\n",
    "\n",
    "    print(f\"Generating interactive 3D plot for {sample_id}...\")\n",
    "\n",
    "    # --- 1. Downsample for stable visualization ---\n",
    "    if downsample_factor < 1.0:\n",
    "        vis_image_3d = scipy.ndimage.zoom(image_3d, downsample_factor, order=1)\n",
    "        vis_gt_mask = scipy.ndimage.zoom(ground_truth_mask_3d, downsample_factor, order=0)\n",
    "        vis_pred_mask = scipy.ndimage.zoom(predicted_mask_3d, downsample_factor, order=0)\n",
    "    else:\n",
    "        vis_image_3d, vis_gt_mask, vis_pred_mask = image_3d, ground_truth_mask_3d, predicted_mask_3d\n",
    "    \n",
    "    gt_binary = (vis_gt_mask > 0.5)\n",
    "    pred_binary = (vis_pred_mask > 0.5)\n",
    "\n",
    "    pio.renderers.default = \"notebook\"\n",
    "    x, y, z = np.mgrid[:vis_image_3d.shape[0], :vis_image_3d.shape[1], :vis_image_3d.shape[2]]\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # --- 2. Add base image volume ---\n",
    "    fig.add_trace(go.Volume(\n",
    "        x=x.flatten(), y=y.flatten(), z=z.flatten(), value=vis_image_3d.flatten(),\n",
    "        isomin=np.min(vis_image_3d), isomax=np.max(vis_image_3d), opacity=0.15,\n",
    "        surface_count=15, colorscale='Greys', name=\"T2W Image\", showscale=False\n",
    "    ))\n",
    "    \n",
    "    # If showing both GT and Prediction, use the 3-color analysis mode\n",
    "    if show_gt and show_pred:\n",
    "        # Calculate Overlap (True Positives), Prediction Only (False Positives), and GT Only (False Negatives)\n",
    "        overlap_mask = gt_binary & pred_binary\n",
    "        pred_only_mask = ~gt_binary & pred_binary\n",
    "        gt_only_mask = gt_binary & ~pred_binary\n",
    "\n",
    "        # Add Overlap region in GREEN\n",
    "        if np.any(overlap_mask):\n",
    "            fig.add_trace(go.Isosurface(\n",
    "                x=x.flatten(), y=y.flatten(), z=z.flatten(), value=overlap_mask.flatten().astype(np.uint8),\n",
    "                isomin=0.5, isomax=1.0, surface_count=1, opacity=0.8,\n",
    "                name=\"Overlap (GT & Pred)\", colorscale=[[0, 'green'], [1, 'green']], showscale=False\n",
    "            ))\n",
    "        \n",
    "        # Add Prediction Only region in RED\n",
    "        if np.any(pred_only_mask):\n",
    "            fig.add_trace(go.Isosurface(\n",
    "                x=x.flatten(), y=y.flatten(), z=z.flatten(), value=pred_only_mask.flatten().astype(np.uint8),\n",
    "                isomin=0.5, isomax=1.0, surface_count=1, opacity=0.7,\n",
    "                name=\"Prediction Only\", colorscale=[[0, 'red'], [1, 'red']], showscale=False\n",
    "            ))\n",
    "\n",
    "        # Add Ground Truth Only region in BLUE\n",
    "        if np.any(gt_only_mask):\n",
    "            fig.add_trace(go.Isosurface(\n",
    "                x=x.flatten(), y=y.flatten(), z=z.flatten(), value=gt_only_mask.flatten().astype(np.uint8),\n",
    "                isomin=0.5, isomax=1.0, surface_count=1, opacity=0.5,\n",
    "                name=\"Ground Truth Only\", colorscale=[[0, 'blue'], [1, 'blue']], showscale=False\n",
    "            ))\n",
    "        \n",
    "        title = \"3D Analysis: Overlap (Green), Prediction Only (Red), GT Only (Blue)\"\n",
    "\n",
    "    # If showing only GT\n",
    "    elif show_gt:\n",
    "        if np.any(gt_binary):\n",
    "            fig.add_trace(go.Isosurface(\n",
    "                x=x.flatten(), y=y.flatten(), z=z.flatten(), value=gt_binary.flatten().astype(np.uint8),\n",
    "                isomin=0.5, isomax=1.0, opacity=0.5, surface_count=1, name=\"Ground Truth\", colorscale='blues', showscale=False\n",
    "            ))\n",
    "        title = \"3D View: Ground Truth\"\n",
    "\n",
    "    # If showing only Prediction\n",
    "    elif show_pred:\n",
    "        if np.any(pred_binary):\n",
    "            fig.add_trace(go.Isosurface(\n",
    "                x=x.flatten(), y=y.flatten(), z=z.flatten(), value=pred_binary.flatten().astype(np.uint8),\n",
    "                isomin=0.5, isomax=1.0, opacity=0.7, surface_count=1, name=\"Prediction\", colorscale='reds', showscale=False\n",
    "            ))\n",
    "        title = \"3D View: Prediction\"\n",
    "    else:\n",
    "        title = \"3D View: Base Image Only\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{title}<br>Sample: {sample_id}\",\n",
    "        scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z', aspectratio=dict(x=1, y=1, z=1), bgcolor='rgb(10, 10, 10)'),\n",
    "        legend=dict(x=0, y=1)\n",
    "    )\n",
    "    \n",
    "    print(\"Displaying plot inline...\")\n",
    "    fig.show()\n",
    "\n",
    "def calculate_3d_metrics(ground_truth, prediction):\n",
    "    gt, pred = (ground_truth > 0.5).astype(bool), (prediction > 0.5).astype(bool)\n",
    "    if not np.any(gt) and not np.any(pred): return {\"dice\": 1.0, \"precision\": 1.0, \"recall\": 1.0}\n",
    "    tp, fp, fn = np.sum(gt & pred), np.sum(~gt & pred), np.sum(gt & ~pred)\n",
    "    epsilon = 1e-8\n",
    "    dice, precision, recall = (2 * tp) / (2 * tp + fp + fn + epsilon), tp / (tp + fp + epsilon), tp / (tp + fn + epsilon)\n",
    "    return {\"dice\": dice, \"precision\": precision, \"recall\": recall} \n",
    "    \n",
    "def predict_evaluate_and_visualize_3d(model, base_dir, sample_id, device, image_size):\n",
    "    print(f\"\\n--- Processing Sample ID: {sample_id} ---\")\n",
    "    model.eval()\n",
    "    modalities = [np.load(os.path.join(base_dir, m, f'{sample_id}.npy')) for m in ['t2w', 'adc', 'hbv']]\n",
    "    image_3d_multichannel = np.stack(modalities, axis=-1)\n",
    "    mask_3d = np.load(os.path.join(base_dir, 'mask', f'{sample_id}.npy'))\n",
    "    prediction_3d = np.zeros_like(mask_3d, dtype=np.float32)\n",
    "    slice_axis = np.argmin(image_3d_multichannel.shape[:-1])\n",
    "    num_slices = image_3d_multichannel.shape[slice_axis]\n",
    "    h, w = (image_3d_multichannel.shape[1], image_3d_multichannel.shape[2]) if slice_axis == 0 else (image_3d_multichannel.shape[0], image_3d_multichannel.shape[2])\n",
    "    val_transform = A.Compose([A.Resize(height=image_size, width=image_size)])\n",
    "    resize_back_transform = A.Compose([A.Resize(height=h, width=w)])\n",
    "    with torch.no_grad():\n",
    "        for slice_idx in tqdm(range(num_slices), desc=f\"Predicting {sample_id}\"):\n",
    "            slicer = [slice(None)] * 3; slicer[slice_axis] = slice_idx\n",
    "            image_slice_np = image_3d_multichannel[tuple(slicer)]\n",
    "            resized_image_np = val_transform(image=image_slice_np.astype(np.float32))['image']\n",
    "            normalized_slice_np = resized_image_np.copy()\n",
    "            for i in range(normalized_slice_np.shape[2]):\n",
    "                channel = normalized_slice_np[:, :, i]\n",
    "                non_zero_pixels = channel[channel > 1e-6]\n",
    "                if non_zero_pixels.size > 0:\n",
    "                    p1, p99 = np.percentile(non_zero_pixels, 1), np.percentile(non_zero_pixels, 99)\n",
    "                    channel = np.clip(channel, p1, p99)\n",
    "                min_val, max_val = channel.min(), channel.max()\n",
    "                normalized_slice_np[:, :, i] = (channel - min_val) / (max_val - min_val) if max_val > min_val else np.zeros_like(channel)\n",
    "            image_tensor = torch.from_numpy(normalized_slice_np.transpose(2, 0, 1)).float().unsqueeze(0).to(device)\n",
    "            with torch.amp.autocast(device_type=str(device)): pred_prob = torch.sigmoid(model(image_tensor))\n",
    "            pred_mask = (pred_prob > 0.5).float().cpu().squeeze().numpy()\n",
    "            resized_pred_mask = resize_back_transform(image=pred_mask)['image']\n",
    "            prediction_3d[tuple(slicer)] = resized_pred_mask\n",
    "\n",
    "    metrics = calculate_3d_metrics(mask_3d, prediction_3d)\n",
    "    print(f\"Metrics for {sample_id}: Dice={metrics['dice']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")\n",
    "    \n",
    "    t2w_image_3d = image_3d_multichannel[:, :, :, 0]\n",
    "    visualize_3d_with_image_overlay(t2w_image_3d, mask_3d, prediction_3d, sample_id, downsample_factor=0.25)\n",
    "    return metrics\n",
    "\n",
    "model = PraNet().to(DEVICE) \n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "num_images_to_predict = 3\n",
    "\n",
    "ids_to_predict = val_ids[10:10+num_images_to_predict]\n",
    "\n",
    "if not ids_to_predict:\n",
    "    print(\"No validation IDs available for 3D prediction.\")\n",
    "else:\n",
    "    print(f\"Will run 3D prediction on the following sample IDs: {ids_to_predict}\")\n",
    "    for sample_id in ids_to_predict:\n",
    "        predict_evaluate_and_visualize_3d(\n",
    "            model=model,\n",
    "            base_dir=DATA_DIR,\n",
    "            sample_id=sample_id,\n",
    "            device=DEVICE,\n",
    "            image_size=IMAGE_SIZE\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7794110,
     "sourceId": 12362055,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
